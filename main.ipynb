{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Core tasks : run the API, get results, store data in a dataframe, vary temperature to get different responses.\n",
    "Token counts should be monitored to avoid exceeding the limit.\n",
    "    \n",
    "Remaining tasks:\n",
    "error handling\n",
    "testing for speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from project_secrets import *   # API key stored in project_secrets.py\n",
    "import pandas as pd\n",
    "\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "def get_response(prompt, data):\n",
    "    for i in range(0, 11, 2):\n",
    "       temperature = i / 10\n",
    "       response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=temperature, max_tokens=15)\n",
    "       new_temp = {\"temp\": temperature}\n",
    "       response.update(new_temp)\n",
    "       data.append(response)\n",
    "\n",
    "def store_response(data):\n",
    "    normalized = pd.json_normalize(data)\n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = normalized['id']\n",
    "    df['temperature'] = normalized['temp']\n",
    "    df['text'] = normalized['choices'][0][0]['text'].strip()\n",
    "    df['logprobs'] = normalized['choices'][0][0]['logprobs']\n",
    "    df['completion_tokens'] = normalized['usage.completion_tokens']\n",
    "    df['prompt_tokens'] = normalized['usage.prompt_tokens']\n",
    "    df['total_tokens'] = normalized['usage.total_tokens']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   id  temperature                       text  \\\n",
      "0  cmpl-79Wy2rKgW3ss8BjdYKOGfI5vfsLcs          0.0  This is just a trial run.   \n",
      "1  cmpl-79Wy3cVPzdtnnJl6R6mRl0JAuujpX          0.2  This is just a trial run.   \n",
      "2  cmpl-79Wy47gXPBi8RPlnBqDkV9lE8fkop          0.4  This is just a trial run.   \n",
      "3  cmpl-79Wy6jIh2zW2x4YZEUBI6XNmjCKUe          0.6  This is just a trial run.   \n",
      "4  cmpl-79Wy7OIvhc30PVDrTL1saZZQNZXMx          0.8  This is just a trial run.   \n",
      "5  cmpl-79Wy9fbVLKBhGacodhNnbPgxzp8pz          1.0  This is just a trial run.   \n",
      "\n",
      "  logprobs  completion_tokens  prompt_tokens  total_tokens  \n",
      "0     None                  9             15            24  \n",
      "1     None                  9             15            24  \n",
      "2     None                  9             15            24  \n",
      "3     None                  9             15            24  \n",
      "4     None                  8             15            23  \n",
      "5     None                  8             15            23  \n"
     ]
    }
   ],
   "source": [
    "# First run - vary temperature on each loop\n",
    "data = []\n",
    "prompt = \"Say something like, 'this is only a test', but change the output\"\n",
    "get_response(prompt, data)\n",
    "\n",
    "foo = store_response(data)\n",
    "print(foo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.0.  Response: 1. \"This is merely a trial run.\"\n",
      "2. \"This is just a practice run.\"\n",
      "Temperature: 0.2.  Response: 1. \"This is merely a test.\"\n",
      "2. \"This is just a trial run.\"\n",
      "Temperature: 0.4.  Response: 1. \"This is merely a trial run.\" \n",
      "2. \"This is just a practice run.\"\n",
      "Temperature: 0.6.  Response: 1. \"This is just a trial run.\"\n",
      "2. \"This is merely a practice run.\"\n",
      "Temperature: 0.8.  Response: 1. \"I'm just running a test here.\"\n",
      "2. \"I'm only trying this out.\"\n",
      "Temperature: 1.0.  Response: 1. I'm just testing this out.\n",
      "2. Let's see how this works.\n"
     ]
    }
   ],
   "source": [
    "# Expected: different results at different temperature. \n",
    "# Actual: same result at all temperatures.\n",
    "# Conclusion: temperature seems to not change across requests.\n",
    "# Next step: Keep changing temperature, but change prompt to ask for more responses with each API call. This requires more max_tokens to capture full set\n",
    "\n",
    "def get_response(prompt):\n",
    "    for i in range(0, 11, 2):\n",
    "        temperature = i / 10\n",
    "        response = openai.Completion.create(model=\"text-davinci-003\", prompt=prompt, temperature=temperature, max_tokens=30)\n",
    "        print(f\"Temperature: {temperature}.  Response: {response.choices[0].text.strip()}\")\n",
    "\n",
    "prompt=\"Say something like, 'this is only a test', but give me two different versions.\"\n",
    "get_response(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
